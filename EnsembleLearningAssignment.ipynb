{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer:\n",
        "Ensemble Learning is a technique in machine learning where multiple models (called base learners) are combined to produce a more powerful model. The key idea is that a group of weak or average models, when combined, can outperform a single strong model.\n",
        "\n",
        "By aggregating predictions, ensemble methods reduce variance, bias, and improve generalization.\n",
        "\n",
        "Example: Random Forest (many decision trees), Gradient Boosting, and Bagging methods.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jDVTruHNXs8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Trains models in parallel on different bootstrap samples of data.\n",
        "\n",
        "Reduces variance, prevents overfitting.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Trains models sequentially, where each new model focuses on correcting errors of the previous ones.\n",
        "\n",
        "Reduces bias, builds strong learners.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n"
      ],
      "metadata": {
        "id": "qA-ANoLpa4Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap Sampling: Random sampling with replacement from the dataset to create multiple training subsets.\n",
        "\n",
        "Role in Bagging:\n",
        "\n",
        "Ensures each model in the ensemble sees a slightly different dataset.\n",
        "\n",
        "Introduces diversity, reducing overfitting.\n",
        "\n",
        "In Random Forest, bootstrap sampling is used to build each decision tree on different samples.\n"
      ],
      "metadata": {
        "id": "1Pruzu9ga0Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "OOB Samples: The data points not included in a bootstrap sample (about 1/3 of the data).\n",
        "\n",
        "OOB Score:\n",
        "\n",
        "Predictions on OOB samples are aggregated across trees.\n",
        "\n",
        "Provides an unbiased estimate of model performance without needing a separate validation set."
      ],
      "metadata": {
        "id": "MVqhW0aIambr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Decision Tree:\n",
        "\n",
        "Feature importance is based on how much each feature decreases impurity (e.g., Gini index).\n",
        "\n",
        "Sensitive to noise and biased toward features with more levels.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Averages feature importance across many trees.\n",
        "\n",
        "More stable and reliable.\n",
        "\n",
        "Reduces variance and provides a more general view of feature contributions.\n",
        "\n"
      ],
      "metadata": {
        "id": "6w5zK-GWahbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6: Python — Random Forest Classifier on Breast Cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Features:\")\n",
        "print(top5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59yFmzOoaF1x",
        "outputId": "6e630a75-3765-4361-8d67-a5006eb7dc44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7: Python — Bagging Classifier vs Single Decision Tree (Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfBIXujFZ671",
        "outputId": "aa76b189-2630-44c1-b0a4-ccf5cfe13b23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8: Python — Random Forest with GridSearchCV from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDcpNnWwZj0b",
        "outputId": "da8e7bd0-7b40-4591-9e58-aa3ba5f73f5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9596180717279925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Q9: Python — Bagging Regressor vs Random Forest Regressor (California Housing)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bag_reg = BaggingRegressor(random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag_reg.predict(X_test))\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W1NmJTOYA1l",
        "outputId": "f2b5c517-4f8a-4a5b-a285-175d16aeff38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2824242776841025\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q10: Loan Default Prediction (Real-world Approach)\n",
        "\n",
        "Answer:\n",
        "Step-by-step approach:\n",
        "\n",
        "Choose Bagging vs Boosting:\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM) works well for imbalanced financial datasets.\n",
        "\n",
        "Bagging (Random Forest) can be tried as a baseline.\n",
        "\n",
        "Handle Overfitting:\n",
        "\n",
        "Use regularization (shrinkage, max_depth, min_child_weight).\n",
        "\n",
        "Apply early stopping for boosting methods.\n",
        "\n",
        "Select Base Models:\n",
        "\n",
        "Decision Trees for both bagging and boosting.\n",
        "\n",
        "Logistic Regression can also be included in ensemble stacking.\n",
        "\n",
        "Cross-validation:\n",
        "\n",
        "Use stratified k-fold cross-validation to evaluate.\n",
        "\n",
        "Monitor metrics like AUC-ROC, F1-score (important for imbalanced classification).\n",
        "\n",
        "Justification:\n",
        "\n",
        "Ensemble methods improve decision-making by combining diverse learners, reducing variance/bias.\n",
        "\n",
        "In loan default prediction, it helps reduce false negatives (missing a default) which is critical in finance."
      ],
      "metadata": {
        "id": "BHTRjp9JX6Or"
      }
    }
  ]
}